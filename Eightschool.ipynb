{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pylab as plt\n",
    "import autograd.numpy as np\n",
    "import seaborn as snb\n",
    "import pystan\n",
    "\n",
    "from autograd import grad\n",
    "\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up Stan model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_19a09b474d1901f191444eaf8a6b8ce2 NOW.\n",
      "/u/80/andersm2/unix/projects/stan_sparse/lib/python3.5/site-packages/Cython/Compiler/Main.py:367: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /tmp/tmp_zhexu0v/stanfit4anon_model_19a09b474d1901f191444eaf8a6b8ce2_397035702896128466.pyx\n",
      "  tree = Parsing.p_module(s, pxd, full_module_name)\n"
     ]
    }
   ],
   "source": [
    "schools_code = \"\"\"\n",
    "data {\n",
    "    int<lower=0> J; // number of schools\n",
    "    vector[J] y; // estimated treatment effects\n",
    "    vector<lower=0>[J] sigma; // s.e. of effect estimates\n",
    "}\n",
    "parameters {\n",
    "    real mu;\n",
    "    real<lower=0> tau;\n",
    "    vector[J] eta;\n",
    "}\n",
    "transformed parameters {\n",
    "    vector[J] theta;\n",
    "    theta = mu + tau * eta;\n",
    "}\n",
    "model {\n",
    "    eta ~ normal(0, 1);\n",
    "    y ~ normal(theta, sigma);\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "J = 8\n",
    "\n",
    "schools_dat = {'J': J,\n",
    "               'y': [28,  8, -3,  7, -1,  1, 18, 12],\n",
    "               'sigma': [15, 10, 16, 11,  9, 11, 10, 18]}\n",
    "\n",
    "sm = pystan.StanModel(model_code=schools_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run HMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:3 of 10000 iterations ended with a divergence (0.03%).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "fit = sm.sampling(data=schools_dat, iter=10000, chains=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Stan-ADVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:Automatic Differentiation Variational Inference (ADVI) is an EXPERIMENTAL ALGORITHM.\n",
      "WARNING:pystan:ADVI samples may be found on the filesystem in the file `/tmp/tmpw5tpp61i/output.csv`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 9.3 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "fit_vb = sm.vb(data=schools_dat, iter=100000, tol_rel_obj=1e-6, output_samples=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run ADVI in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.25 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "# Helper functions\n",
    "def gaussian_entropy(log_sigma):\n",
    "    return 0.5*(np.log(2*np.pi) + 2*log_sigma + 1.)\n",
    "\n",
    "def compute_entropy(log_sigma):\n",
    "    return np.sum(gaussian_entropy(log_sigma))\n",
    "\n",
    "def reparametrize(zs, means, log_sigmas):\n",
    "    samples = means[:, None] + np.exp(log_sigmas[:, None])*zs \n",
    "    log_sigma_grad = (sigmas[:, None]*zs).T\n",
    "    return samples, log_sigma_grad\n",
    "\n",
    "# use analytical gradient of entropy\n",
    "compute_entropy_grad = grad(compute_entropy)\n",
    "    \n",
    "# settings\n",
    "step_size = 1e-2\n",
    "itt_max = 10000\n",
    "num_samples = 1\n",
    "\n",
    "# init variational params\n",
    "num_params = 10\n",
    "means = np.zeros(num_params)\n",
    "sigmas = np.ones(num_params)\n",
    "log_sigmas = np.log(sigmas)\n",
    "\n",
    "\n",
    "# Optimize\n",
    "for itt in range(itt_max):\n",
    "    \n",
    "    # generate samples from q \n",
    "    zs = np.random.normal(0, 1, size=(num_params, num_samples))\n",
    "    samples, grad_correction = reparametrize(zs, means, log_sigmas)\n",
    "        \n",
    "    # evaluate gradient of log p (does grad_log_prob support vectorization??) and gradient of log q\n",
    "    log_p_grad = np.array([fit.grad_log_prob(s) for s in samples.T])\n",
    "    entropy_grad = compute_entropy_grad(log_sigmas)\n",
    "    \n",
    "    # compute gradients wrt. mean and log_sigma\n",
    "    mean_grad = np.mean(log_p_grad, axis=0)\n",
    "    log_sigma_grad =np.mean(log_p_grad*grad_correction, axis=0) + entropy_grad\n",
    "    \n",
    "    # evaluate ELBO\n",
    "    #log_p = np.array([fit.log_prob(theta_i) for theta_i in thetas.T])\n",
    "    #ELBO = np.mean(log_p) + compute_entropy(log_sigmas)\n",
    "    \n",
    "    # take gradient step\n",
    "    means += step_size*mean_grad\n",
    "    log_sigmas += step_size*log_sigma_grad\n",
    "    \n",
    "    #  transform back to constrained space\n",
    "    sigmas = np.exp(log_sigmas)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare sample from each of the three posterior distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# HMC\n",
    "la = fit.extract(permuted=True)\n",
    "stan_eta = la['eta']\n",
    "stan_tau = la['tau']\n",
    "stan_mu = la['mu']\n",
    "stan_theta = stan_mu[:, None] + stan_tau[:, None]*stan_eta\n",
    "\n",
    "# Stan-VB\n",
    "fit_vb_samples = np.array(fit_vb['sampler_params']).T\n",
    "stan_vb_mu = fit_vb_samples[:, 0]\n",
    "stan_vb_tau = fit_vb_samples[:, 1]\n",
    "stan_vb_eta = fit_vb_samples[:, 2:10]\n",
    "stan_vb_theta = fit_vb_samples[:, 10:18]\n",
    "\n",
    "\n",
    "# VB\n",
    "Q = 5000\n",
    "vb_mu = np.random.normal(means[0], sigmas[0], size=(Q))\n",
    "vb_tau = np.exp(np.random.normal(means[1], sigmas[1], size=(Q)))\n",
    "vb_eta = np.random.normal(means[2:, None], sigmas[2:, None], size=(J, Q)).T\n",
    "vb_theta = vb_mu[:, None] + vb_tau[:, None]*vb_eta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snb.set(font_scale=1.5)\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(stan_mu, 50, histtype='step', linewidth=3, label='HMC', density=True);\n",
    "plt.hist(stan_vb_mu, 50, histtype='step', linewidth=3, label='Stan-VB', density=True);\n",
    "plt.hist(vb_mu, 50, histtype='step', linewidth=3, label='Python-VB', density=True);\n",
    "plt.legend()\n",
    "plt.xlabel('$\\mu$')\n",
    "plt.title('Paramter: $\\mu$', fontweight='bold')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(np.log(stan_tau), 50, histtype='step', linewidth=3, label='HMC', density=True);\n",
    "plt.hist(np.log(stan_vb_tau), 50, histtype='step', linewidth=3, label='Stan-VB', density=True);\n",
    "plt.hist(np.log(vb_tau), 50, histtype='step', linewidth=3, label='Python-VB', density=True);\n",
    "plt.legend()\n",
    "plt.xlabel('$\\ln \\\\tau$')\n",
    "plt.title('Paramter: $\\ln \\\\tau$', fontweight='bold')\n",
    "\n",
    "plt.figure(figsize=(20, 30))\n",
    "for j in range(J):\n",
    "    plt.subplot(4, 2, 1 + j)\n",
    "    plt.hist(stan_theta[:, j], 50, histtype='step', linewidth=3, label='HMC', density=True);\n",
    "    plt.hist(stan_vb_theta[:, j], 50, histtype='step', linewidth=3, label='Stan-VB', density=True);\n",
    "    plt.hist(vb_theta[:, j], 50, histtype='step', linewidth=3, label='Python-VB', density=True);\n",
    "    plt.legend()\n",
    "    plt.xlabel('$\\\\theta_{%d}$' % (j+1))\n",
    "    plt.title('Paramter: $\\\\theta_{%d}$' % (j+1), fontweight='bold')\n",
    "                   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funnel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.log(stan_tau), stan_theta[:, 0], 'b.', label='HMC')\n",
    "plt.plot(np.log(vb_tau), vb_theta[:, 0], 'r.', label='Python-VB')\n",
    "plt.plot(np.log(stan_vb_tau), stan_vb_theta[:, 0], 'g.', label='Stan-VB')\n",
    "\n",
    "plt.xlabel('Log $\\\\tau$')\n",
    "plt.ylabel('$\\\\theta_1$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
